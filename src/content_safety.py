"""Content Safety Layer — pre-filter for extreme or out-of-scope content.

Evaluates user input (text + media descriptions) before classification
to reject content that is unrelated to agency operations.
"""
from __future__ import annotations

import json
import logging
from dataclasses import dataclass

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

from src.config import MODEL_NAME, MODEL_TEMPERATURE
from src.prompts.loader import render

logger = logging.getLogger(__name__)

_safety_llm: ChatOpenAI | None = None


def _get_safety_llm() -> ChatOpenAI:
    global _safety_llm
    if _safety_llm is None:
        _safety_llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
    return _safety_llm


@dataclass
class ContentSafetyResult:
    is_safe: bool
    reason: str


def check_content_safety(
    text: str,
    media_descriptions: list[str] | None = None,
) -> ContentSafetyResult:
    """Evaluate whether user content is safe and relevant for the incident system.

    Args:
        text: User text description.
        media_descriptions: Optional list of image/audio descriptions already
            generated by the media processor.

    Returns:
        ContentSafetyResult with is_safe=True if content can proceed, False otherwise.
    """
    # Build the full content string to evaluate
    parts = [text] if text else []
    for desc in media_descriptions or []:
        if desc:
            parts.append(f"[Descripción de media adjunta: {desc}]")

    content = "\n".join(parts).strip()

    if not content:
        # Empty content is safe — the classifier will handle it
        return ContentSafetyResult(is_safe=True, reason="Contenido vacío")

    prompt = render("content_safety.j2", content=content)

    try:
        resp = _get_safety_llm().invoke([HumanMessage(content=prompt)])
        data = json.loads(resp.content.strip())
        verdict = data.get("verdict", "SAFE").upper()
        reason = data.get("reason", "")
    except (json.JSONDecodeError, AttributeError, Exception) as e:
        # On any error, default to SAFE to avoid blocking legitimate reports
        logger.warning("Content safety check failed, defaulting to SAFE: %s", e)
        return ContentSafetyResult(is_safe=True, reason="Error en evaluación, se permite por defecto")

    is_safe = verdict == "SAFE"
    if not is_safe:
        logger.info("Content flagged as UNSAFE: %s", reason)

    return ContentSafetyResult(is_safe=is_safe, reason=reason)
